{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtablado/uoc2022_tfm/blob/main/vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://learnopencv.com/variational-autoencoder-in-tensorflow/\n",
        "\n",
        "https://keras.io/examples/generative/vae/"
      ],
      "metadata": {
        "id": "yHasBMHRxHb8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_dhLG4xZbM9y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adadelta, Adagrad\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)"
      ],
      "metadata": {
        "id": "edtgkvZzbWoW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJFQ5I2cbZtm",
        "outputId": "546884e9-8663-4227-c36d-0a259bca43dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "9el8GlkAbaS0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_encoder():\n",
        "  latent_dim = 2\n",
        "\n",
        "  inputs = keras.Input(shape=(256, 256, 1), name='input_layer')\n",
        "\n",
        "  # Block-1\n",
        "  x = layers.Conv2D(32, kernel_size=3, strides= 2, padding='same', name='conv_1')(inputs)\n",
        "  x = layers.BatchNormalization(name='bn_1')(x)\n",
        "  x = layers.LeakyReLU(name='lrelu_1')(x)\n",
        "\n",
        "  # Block-2\n",
        "  x = layers.Conv2D(64, kernel_size=3, strides= 2, padding='same', name='conv_2')(x)\n",
        "  x = layers.BatchNormalization(name='bn_2')(x)\n",
        "  x = layers.LeakyReLU(name='lrelu_2')(x)\n",
        "\n",
        "  # Block-3\n",
        "  x = layers.Conv2D(64, 3, 2, padding='same', name='conv_3')(x)\n",
        "  x = layers.BatchNormalization(name='bn_3')(x)\n",
        "  x = layers.LeakyReLU(name='lrelu_3')(x)\n",
        "\n",
        "  # Block-4\n",
        "  x = layers.Conv2D(64, 3, 2, padding='same', name='conv_4')(x)\n",
        "  x = layers.BatchNormalization(name='bn_4')(x)\n",
        "  x = layers.LeakyReLU(name='lrelu_4')(x)\n",
        "\n",
        "  # Block-5\n",
        "  x = layers.Conv2D(64, 3, 2, padding='same', name='conv_5')(x)\n",
        "  x = layers.BatchNormalization(name='bn_5')(x)\n",
        "  x = layers.LeakyReLU(name='lrelu_5')(x)\n",
        "\n",
        "  # Final Block\n",
        "  flatten = layers.Flatten()(x)\n",
        "  mean = layers.Dense(200, name='mean')(flatten)\n",
        "  log_var = layers.Dense(200, name='log_var')(flatten)\n",
        "  z = Sampling()([mean, log_var])\n",
        "  model = tf.keras.Model(inputs, (mean, log_var, z), name=\"Encoder\")\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "my_encoder = vae_encoder()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUVpfaJIbkj2",
        "outputId": "ffa84adf-0839-4a16-c0f9-3120a56d393b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_layer (InputLayer)       [(None, 256, 256, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv_1 (Conv2D)                (None, 128, 128, 32  320         ['input_layer[0][0]']            \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " bn_1 (BatchNormalization)      (None, 128, 128, 32  128         ['conv_1[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " lrelu_1 (LeakyReLU)            (None, 128, 128, 32  0           ['bn_1[0][0]']                   \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv_2 (Conv2D)                (None, 64, 64, 64)   18496       ['lrelu_1[0][0]']                \n",
            "                                                                                                  \n",
            " bn_2 (BatchNormalization)      (None, 64, 64, 64)   256         ['conv_2[0][0]']                 \n",
            "                                                                                                  \n",
            " lrelu_2 (LeakyReLU)            (None, 64, 64, 64)   0           ['bn_2[0][0]']                   \n",
            "                                                                                                  \n",
            " conv_3 (Conv2D)                (None, 32, 32, 64)   36928       ['lrelu_2[0][0]']                \n",
            "                                                                                                  \n",
            " bn_3 (BatchNormalization)      (None, 32, 32, 64)   256         ['conv_3[0][0]']                 \n",
            "                                                                                                  \n",
            " lrelu_3 (LeakyReLU)            (None, 32, 32, 64)   0           ['bn_3[0][0]']                   \n",
            "                                                                                                  \n",
            " conv_4 (Conv2D)                (None, 16, 16, 64)   36928       ['lrelu_3[0][0]']                \n",
            "                                                                                                  \n",
            " bn_4 (BatchNormalization)      (None, 16, 16, 64)   256         ['conv_4[0][0]']                 \n",
            "                                                                                                  \n",
            " lrelu_4 (LeakyReLU)            (None, 16, 16, 64)   0           ['bn_4[0][0]']                   \n",
            "                                                                                                  \n",
            " conv_5 (Conv2D)                (None, 8, 8, 64)     36928       ['lrelu_4[0][0]']                \n",
            "                                                                                                  \n",
            " bn_5 (BatchNormalization)      (None, 8, 8, 64)     256         ['conv_5[0][0]']                 \n",
            "                                                                                                  \n",
            " lrelu_5 (LeakyReLU)            (None, 8, 8, 64)     0           ['bn_5[0][0]']                   \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 4096)         0           ['lrelu_5[0][0]']                \n",
            "                                                                                                  \n",
            " mean (Dense)                   (None, 200)          819400      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " log_var (Dense)                (None, 200)          819400      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " sampling (Sampling)            (None, 200)          0           ['mean[0][0]',                   \n",
            "                                                                  'log_var[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,769,552\n",
            "Trainable params: 1,768,976\n",
            "Non-trainable params: 576\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_decoder():\n",
        "  inputs = keras.Input(shape=(200,), name='input_layer')\n",
        "  x = layers.Dense(4096, name='dense_1')(inputs)\n",
        "  x = layers.Reshape((8,8,64), name='Reshape')(x)\n",
        "\n",
        "  # Block-1\n",
        "  x = layers.Conv2DTranspose(64, 3, strides= 2, padding='same',name='conv_transpose_1')(x)\n",
        "  x = layers.BatchNormalization(name='bn_1')(x)\n",
        "  x = layers.LeakyReLU(name='lrelu_1')(x)\n",
        "\n",
        "  # Block-2\n",
        "  x = layers.Conv2DTranspose(64, 3, strides= 2, padding='same', name='conv_transpose_2')(x)\n",
        "  x = layers.BatchNormalization(name='bn_2')(x)\n",
        "  x = layers.LeakyReLU(name='lrelu_2')(x)\n",
        "\n",
        "  # Block-3\n",
        "  x = layers.Conv2DTranspose(64, 3, 2, padding='same', name='conv_transpose_3')(x)\n",
        "  x = layers.BatchNormalization(name='bn_3')(x)\n",
        "  x = layers.LeakyReLU(name='lrelu_3')(x)\n",
        "\n",
        "  # Block-4\n",
        "  x = layers.Conv2DTranspose(32, 3, 2, padding='same', name='conv_transpose_4')(x)\n",
        "  x = layers.BatchNormalization(name='bn_4')(x)\n",
        "  x = layers.LeakyReLU(name='lrelu_4')(x)\n",
        "\n",
        "  # Block-5\n",
        "  outputs = layers.Conv2DTranspose(1, 3, 2,padding='same', activation='sigmoid', name='conv_transpose_5')(x)\n",
        "  model = tf.keras.Model(inputs, outputs, name=\"Decoder\")\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "my_decoder = vae_decoder()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAvh0mpMbn0i",
        "outputId": "9c649bb6-5122-4503-c1a4-56a3f47bbcd6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_layer (InputLayer)    [(None, 200)]             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4096)              823296    \n",
            "                                                                 \n",
            " Reshape (Reshape)           (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv_transpose_1 (Conv2DTra  (None, 16, 16, 64)       36928     \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " bn_1 (BatchNormalization)   (None, 16, 16, 64)        256       \n",
            "                                                                 \n",
            " lrelu_1 (LeakyReLU)         (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " conv_transpose_2 (Conv2DTra  (None, 32, 32, 64)       36928     \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " bn_2 (BatchNormalization)   (None, 32, 32, 64)        256       \n",
            "                                                                 \n",
            " lrelu_2 (LeakyReLU)         (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv_transpose_3 (Conv2DTra  (None, 64, 64, 64)       36928     \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " bn_3 (BatchNormalization)   (None, 64, 64, 64)        256       \n",
            "                                                                 \n",
            " lrelu_3 (LeakyReLU)         (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv_transpose_4 (Conv2DTra  (None, 128, 128, 32)     18464     \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " bn_4 (BatchNormalization)   (None, 128, 128, 32)      128       \n",
            "                                                                 \n",
            " lrelu_4 (LeakyReLU)         (None, 128, 128, 32)      0         \n",
            "                                                                 \n",
            " conv_transpose_5 (Conv2DTra  (None, 256, 256, 1)      289       \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 953,729\n",
            "Trainable params: 953,281\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        result = {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "        return result \n"
      ],
      "metadata": {
        "id": "zmxHbPG4brFe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/tfm/dataset/img\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        'training',\n",
        "        target_size=(256, 256),\n",
        "        batch_size=32,\n",
        "        color_mode='grayscale',\n",
        "        class_mode=None)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        'test',\n",
        "        target_size=(256, 256),\n",
        "        batch_size=32,\n",
        "        color_mode='grayscale',\n",
        "        class_mode=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naTJxG4kbtfA",
        "outputId": "ffe19673-a5e4-46c3-f689-2b7463989745"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1yKlrvb9Cp3amASiDUettfP3V1KJuzGDe/tfm/dataset/slices\n",
            "Found 0 images belonging to 0 classes.\n",
            "Found 0 images belonging to 0 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "vae = VAE(my_encoder, my_decoder)\n",
        "vae.compile(\n",
        "    optimizer=keras.optimizers.Adam())\n",
        "\n",
        "# Entrenar el modelo\n",
        "t0 = timeit.default_timer()\n",
        "\n",
        "mfit = vae.fit(train_generator, epochs=30, batch_size=62)\n",
        "training_time_ann = timeit.default_timer() - t0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "FcpHzj4lbv-m",
        "outputId": "f3ed019a-dcb8-4209-89f8-eb486f586ffb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a370f8036ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m62\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtraining_time_ann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m       raise ValueError('Asked to retrieve element {idx}, '\n\u001b[0m\u001b[1;32m    101\u001b[0m                        \u001b[0;34m'but the Sequence '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                        'has length {length}'.format(idx=idx, length=len(self)))\n",
            "\u001b[0;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean, log_var, z = my_encoder.predict(test_generator)\n",
        "y_pred = my_decoder.predict(z)"
      ],
      "metadata": {
        "id": "pQmE3c9IffDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "fig, axes = plt.subplots(1, len(y_pred), figsize=(25,25))\n",
        "\n",
        "for i in range(len(y_pred)):\n",
        "  image = y_pred[1]\n",
        "  image = image[:,:,0]\n",
        "  axes[i].imshow(image, cmap=\"gray\", origin=\"lower\")\n",
        "  # Display the first image in training data\n",
        "  #img = image[:,:,100]\n",
        "  plt.imshow(image, cmap='gray')"
      ],
      "metadata": {
        "id": "2WzhFpp3fn80"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}