{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUgcDjk3ETs608aRp+wnOr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtablado/uoc2022_tfm/blob/main/vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_dhLG4xZbM9y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adadelta, Adagrad\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)"
      ],
      "metadata": {
        "id": "edtgkvZzbWoW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJFQ5I2cbZtm",
        "outputId": "cc33efa9-681b-4d7e-cc32-e0a02829de56"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "9el8GlkAbaS0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_encoder():\n",
        "  latent_dim = 2\n",
        "\n",
        "  inputs = keras.Input(shape=(256, 256, 1), name='input_layer')\n",
        "\n",
        "  # Block-1\n",
        "  x = layers.Conv2D(32, kernel_size=3, strides= 2, padding='same', name='conv_1')(inputs)\n",
        "  x = layers.BatchNormalization(name='bn_1')(x)\n",
        "  x = layers.LeakyReLU(name='lrelu_1')(x)\n",
        "\n",
        "  # Block-2\n",
        "  x = layers.Conv2D(64, kernel_size=3, strides= 2, padding='same', name='conv_2')(x)\n",
        "  x = layers.BatchNormalization(name='bn_2')(x)\n",
        "  x = layers.LeakyReLU(name='lrelu_2')(x)\n",
        "\n",
        "  # Block-3\n",
        "  x = layers.Conv2D(64, 3, 2, padding='same', name='conv_3')(x)\n",
        "  x = layers.BatchNormalization(name='bn_3')(x)\n",
        "  x = layers.LeakyReLU(name='lrelu_3')(x)\n",
        "\n",
        "  # Block-4\n",
        "  x = layers.Conv2D(64, 3, 2, padding='same', name='conv_4')(x)\n",
        "  x = layers.BatchNormalization(name='bn_4')(x)\n",
        "  x = layers.LeakyReLU(name='lrelu_4')(x)\n",
        "\n",
        "  # Block-5\n",
        "  x = layers.Conv2D(64, 3, 2, padding='same', name='conv_5')(x)\n",
        "  x = layers.BatchNormalization(name='bn_5')(x)\n",
        "  x = layers.LeakyReLU(name='lrelu_5')(x)\n",
        "\n",
        "  # Final Block\n",
        "  flatten = layers.Flatten()(x)\n",
        "  mean = layers.Dense(200, name='mean')(flatten)\n",
        "  log_var = layers.Dense(200, name='log_var')(flatten)\n",
        "  z = Sampling()([mean, log_var])\n",
        "  model = tf.keras.Model(inputs, (mean, log_var, z), name=\"Encoder\")\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "my_encoder = vae_encoder()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUVpfaJIbkj2",
        "outputId": "70c445dc-80c6-463f-c25b-47639f828894"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_layer (InputLayer)       [(None, 256, 256, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv_1 (Conv2D)                (None, 128, 128, 32  320         ['input_layer[0][0]']            \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " bn_1 (BatchNormalization)      (None, 128, 128, 32  128         ['conv_1[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " lrelu_1 (LeakyReLU)            (None, 128, 128, 32  0           ['bn_1[0][0]']                   \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv_2 (Conv2D)                (None, 64, 64, 64)   18496       ['lrelu_1[0][0]']                \n",
            "                                                                                                  \n",
            " bn_2 (BatchNormalization)      (None, 64, 64, 64)   256         ['conv_2[0][0]']                 \n",
            "                                                                                                  \n",
            " lrelu_2 (LeakyReLU)            (None, 64, 64, 64)   0           ['bn_2[0][0]']                   \n",
            "                                                                                                  \n",
            " conv_3 (Conv2D)                (None, 32, 32, 64)   36928       ['lrelu_2[0][0]']                \n",
            "                                                                                                  \n",
            " bn_3 (BatchNormalization)      (None, 32, 32, 64)   256         ['conv_3[0][0]']                 \n",
            "                                                                                                  \n",
            " lrelu_3 (LeakyReLU)            (None, 32, 32, 64)   0           ['bn_3[0][0]']                   \n",
            "                                                                                                  \n",
            " conv_4 (Conv2D)                (None, 16, 16, 64)   36928       ['lrelu_3[0][0]']                \n",
            "                                                                                                  \n",
            " bn_4 (BatchNormalization)      (None, 16, 16, 64)   256         ['conv_4[0][0]']                 \n",
            "                                                                                                  \n",
            " lrelu_4 (LeakyReLU)            (None, 16, 16, 64)   0           ['bn_4[0][0]']                   \n",
            "                                                                                                  \n",
            " conv_5 (Conv2D)                (None, 8, 8, 64)     36928       ['lrelu_4[0][0]']                \n",
            "                                                                                                  \n",
            " bn_5 (BatchNormalization)      (None, 8, 8, 64)     256         ['conv_5[0][0]']                 \n",
            "                                                                                                  \n",
            " lrelu_5 (LeakyReLU)            (None, 8, 8, 64)     0           ['bn_5[0][0]']                   \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 4096)         0           ['lrelu_5[0][0]']                \n",
            "                                                                                                  \n",
            " mean (Dense)                   (None, 200)          819400      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " log_var (Dense)                (None, 200)          819400      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " sampling (Sampling)            (None, 200)          0           ['mean[0][0]',                   \n",
            "                                                                  'log_var[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,769,552\n",
            "Trainable params: 1,768,976\n",
            "Non-trainable params: 576\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_decoder():\n",
        "  inputs = keras.Input(shape=(200,), name='input_layer')\n",
        "  x = layers.Dense(4096, name='dense_1')(inputs)\n",
        "  x = layers.Reshape((8,8,64), name='Reshape')(x)\n",
        "\n",
        "  # Block-1\n",
        "  x = layers.Conv2DTranspose(64, 3, strides= 2, padding='same',name='conv_transpose_1')(x)\n",
        "  x = layers.BatchNormalization(name='bn_1')(x)\n",
        "  x = layers.LeakyReLU(name='lrelu_1')(x)\n",
        "\n",
        "  # Block-2\n",
        "  x = layers.Conv2DTranspose(64, 3, strides= 2, padding='same', name='conv_transpose_2')(x)\n",
        "  x = layers.BatchNormalization(name='bn_2')(x)\n",
        "  x = layers.LeakyReLU(name='lrelu_2')(x)\n",
        "\n",
        "  # Block-3\n",
        "  x = layers.Conv2DTranspose(64, 3, 2, padding='same', name='conv_transpose_3')(x)\n",
        "  x = layers.BatchNormalization(name='bn_3')(x)\n",
        "  x = layers.LeakyReLU(name='lrelu_3')(x)\n",
        "\n",
        "  # Block-4\n",
        "  x = layers.Conv2DTranspose(32, 3, 2, padding='same', name='conv_transpose_4')(x)\n",
        "  x = layers.BatchNormalization(name='bn_4')(x)\n",
        "  x = layers.LeakyReLU(name='lrelu_4')(x)\n",
        "\n",
        "  # Block-5\n",
        "  outputs = layers.Conv2DTranspose(1, 3, 2,padding='same', activation='sigmoid', name='conv_transpose_5')(x)\n",
        "  model = tf.keras.Model(inputs, outputs, name=\"Decoder\")\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "my_decoder = vae_decoder()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAvh0mpMbn0i",
        "outputId": "3596b567-9dd1-40f7-9fdc-d8ed7837f6d7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_layer (InputLayer)    [(None, 200)]             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4096)              823296    \n",
            "                                                                 \n",
            " Reshape (Reshape)           (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv_transpose_1 (Conv2DTra  (None, 16, 16, 64)       36928     \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " bn_1 (BatchNormalization)   (None, 16, 16, 64)        256       \n",
            "                                                                 \n",
            " lrelu_1 (LeakyReLU)         (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " conv_transpose_2 (Conv2DTra  (None, 32, 32, 64)       36928     \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " bn_2 (BatchNormalization)   (None, 32, 32, 64)        256       \n",
            "                                                                 \n",
            " lrelu_2 (LeakyReLU)         (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv_transpose_3 (Conv2DTra  (None, 64, 64, 64)       36928     \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " bn_3 (BatchNormalization)   (None, 64, 64, 64)        256       \n",
            "                                                                 \n",
            " lrelu_3 (LeakyReLU)         (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv_transpose_4 (Conv2DTra  (None, 128, 128, 32)     18464     \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " bn_4 (BatchNormalization)   (None, 128, 128, 32)      128       \n",
            "                                                                 \n",
            " lrelu_4 (LeakyReLU)         (None, 128, 128, 32)      0         \n",
            "                                                                 \n",
            " conv_transpose_5 (Conv2DTra  (None, 256, 256, 1)      289       \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 953,729\n",
            "Trainable params: 953,281\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        result = {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "        print(result)\n",
        "        return result \n"
      ],
      "metadata": {
        "id": "zmxHbPG4brFe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/tfm/dataset/images\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        'training',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naTJxG4kbtfA",
        "outputId": "55bc0deb-0904-48fd-9b91-167881b2c150"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/tfm/dataset/images\n",
            "Found 62 images belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "vae = VAE(my_encoder, my_decoder)\n",
        "vae.compile(\n",
        "    optimizer=keras.optimizers.Adam())\n",
        "\n",
        "# Entrenar el modelo\n",
        "t0 = timeit.default_timer()\n",
        "\n",
        "mfit = vae.fit(train_generator, epochs=30, batch_size=62)\n",
        "training_time_ann = timeit.default_timer() - t0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FcpHzj4lbv-m",
        "outputId": "567102e4-52bc-4c09-da91-5f17ad6b4f93"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "{'loss': <tf.Tensor 'Identity:0' shape=() dtype=float32>, 'reconstruction_loss': <tf.Tensor 'Identity_1:0' shape=() dtype=float32>, 'kl_loss': <tf.Tensor 'Identity_2:0' shape=() dtype=float32>}\n",
            "{'loss': <tf.Tensor 'Identity:0' shape=() dtype=float32>, 'reconstruction_loss': <tf.Tensor 'Identity_1:0' shape=() dtype=float32>, 'kl_loss': <tf.Tensor 'Identity_2:0' shape=() dtype=float32>}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a370f8036ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m62\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtraining_time_ann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'Encoder/conv_1/BiasAdd' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 149, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py\", line 690, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py\", line 743, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 787, in inner\n      self.run()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 748, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 543, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2854, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3057, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-10-a370f8036ef0>\", line 9, in <module>\n      mfit = vae.fit(train_generator, epochs=30, batch_size=62)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"<ipython-input-7-c65717339b8a>\", line 22, in train_step\n      z_mean, z_log_var, z = self.encoder(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 458, in call\n      return self._run_internal_graph(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/layers/convolutional/base_conv.py\", line 269, in call\n      outputs = tf.nn.bias_add(\nNode: 'Encoder/conv_1/BiasAdd'\nFused conv implementation does not support grouped convolutions for now.\n\t [[{{node Encoder/conv_1/BiasAdd}}]] [Op:__inference_train_function_3411]"
          ]
        }
      ]
    }
  ]
}