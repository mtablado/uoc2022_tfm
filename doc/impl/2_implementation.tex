\section{Phase 2: MR Images creation}

\subsection{Variational Autoencoder}

The source code of the implemented VAE can be found at \href{https://github.com/mtablado/uoc2022_tfm/blob/main/vae.ipynb}{vae.ipynb} file inside the project repository and it is inspired by Variational Autoencoder \cite{vaekeras} keras example.

\begin{figure}[ht]
    \centering
    \includegraphics[width = 16cm, height = 6cm]{images/vae.png}
    \caption[Diagram of Implemented VAE Architecture]{Diagram of Implemented VAE Architecture}
    \label{fig:vaearch}
\end{figure}

The encoder consists of a \acrfull*{cnn} that uses the preprocessed brain slices images as input and then two vectors that form the latent space, which has been sized to store 1024 attributes. \acrshort*{cnn} implementation is described later in a dedicated section.

Encoder output vectors are:

\begin{itemize}
    \item A 1024-dimensional vector of means ($\mu$), and
    \item a 1024-dimensional vector of standard deviations (z) as $log(z)$ to make sure they are positive.
\end{itemize}
 
Then those vectors are sampled and fed into the decoder to reconstruct the \acrshort{mri}. Backpropagation is applied to correct latent space issues and let the model be more accurate.

To implement above architecture in Keras, a new model was created by overriding the Keras Model abstract class and implementing the methods that the interface requires. So, specific methods are coded to process each and every train step and the metrics property is overriden to store the training loss, the reconstruction loss and the Kullback-Leibler divergence data.

The training method calculates the reconstruction loss and the KL divergence. Then, their values are aggregated to result in the training loss value.

\begin{equation}
    Training Loss = Reconstruction Loss + KL Divergence
\end{equation}

The reconstruction loss measures how close the encoder input and the encoder output are by calculating the \acrshort{mse} of the difference between images. On the other hand the KL-Divergence measures the distance between two probability distributions. So, minimizing the KL loss in this case means ensuring that the learnt means and variances are as close as possible to those of the target (normal) distribution.

The latent space of the VAE is usually a Gaussian distribution. Gaussian distribution is also used here since it has very good computational properties. 

The sampling process is implemented as an encoder Keras Layer which takes both vectors and samples with a formula that aggregates the mean and the variance which adjusted with a random factor.


\subsubsection*{The KL-Divergence issue}

As introduced in \ref*{sec:imploverview} overview section, some deviations from the project happened. The KL-Divergence interpretation and calculation was (and remains) a problem. Contrary to what should happen, instead of decreasing, the KL-Divergence metric increases along the epochs. 

Several calculation formulas for KL-Divergence were tested with little result changes (they are commented in at source file). 

The best model accuracy comes at maximizing the \acrfull*{elbo}, which happens when you minimize both reconstruction loss and KL-Divergence.

Despite the KL-Divergence issue, the \acrshort*{elbo} improves in each step thanks to the higher minimization of the reconstruction loss

\begin{equation}
    \left\lvert Reconstruction Loss_i - Reconstruction Loss_{i-1} \right\rvert > \left\lvert KL Divergence_i - KL Divergence_{i-1} \right\rvert
\end{equation}
 
This casualty is allowing the model to keep learning.

This issue inclined the project to create a secondary approach, train also a \acrshort*{vqvae} model to compare results which is describe in the following section, since the time constraint did not allow the project to keep investigating on this issue.

\subsection{Vector-Quantized Variational Autoencoder}

The source code of this section can be found at \href{https://github.com/mtablado/uoc2022_tfm/blob/main/vq-vae.ipynb}{vq-vae.ipynb} file inside the project repository and it is inspired by Vector-Quantized Variational Autoencoder \cite{vqvaekeras} keras example, which code was used as starting point.

\begin{figure}[ht]
    \centering
    \includegraphics[width = 16cm, height = 8cm]{images/vq-vae.png}
    \caption[Diagram of Implemented VQ-VAE Architecture]{Diagram of Implemented VQ-VAE Architecture}
    \label{fig:vqvaearch}
\end{figure}

As VAE, \acrshort*{vqvae} use \acrshort*{cnn} but, in this case, the encoder generates a set of vectors with discrete representations (instead of continuous like VAE). During training phase VQ-VAE creates a codebook of vectors which are also learnable and predictable. Once that a new image is processed, the encoder output vector of z is compared with the codebook vectors and the one closer to it is used later as the input of the decoder. Proximity between vectors is calculated with the L2-norm.

The resulting indexes are stored into a set that then serves as the indexing object to build a new set of vectors that will be used as decoder input.

To do above-mentioned steps, a Vector Quantizer object (a new type of Keras Layer) was created to select the quantized vectors from the latent space which are closer to the encoder output vectors and then, those quantized vectors will be used as decoder inputs.

To implement above architecture in Keras, as like for VAEs and along with the Vector Quantized object, a new model was created by overriding the Keras Model abstract class and implementing the methods that the interface requires. So, specific methods are coded to process each and every train step and the metrics property is overriden to store the total loss, the reconstruction loss and the VQ loss.

The training method calculates the reconstruction loss and the VQ loss and uses them to calculate the training loss (total training here) with the following formula:

\begin{equation}
    Total Loss = Reconstruction Loss + \sum (VQ losses)
\end{equation}

\subsection{CNN Network Architectures}

The implemented \acrshort{cnn} is based on Context-encoding Variational Autoencoder for Unsupervised Anomaly Detection paper \cite{cevaemodel} in where Zimmerer et al. describe it as: 

\begin{quote}
    For the encoder and decoder networks, we chose fully convolutional networks with five 2D-Conv-Layers and 2D-Transposed-Conv-Layers respectively with CoordConv, kernel size 4 and stride 2, each layer followed by a Leaky-ReLU non-linearity. The encoder and decoder are symmetric with 16, 64, 256, 1024 feature maps and a latent variable size of 1024
\end{quote}

Figure \ref{fig:cnnencoderoutput} shows the encoder model summary output after compiling the model. It is truncated to show only CNN-related layers removing VAE or VQ-VAE specific layers. Full output can be checked at Figures \ref{fig:cnnvaeencoderoutput} and \ref{fig:cnnvqvaeencoderoutput}

\begin{figure}[ht]
    \centering
    \includegraphics[width = 14cm, height = 8cm]{images/cnn-encoder-output.png}
    \caption[CNN Encoder model summary output]{CNN Encoder model summary output}
    \label{fig:cnnencoderoutput}
\end{figure}

The input size of the \acrshort*{cnn} is [128,128,1] which are the 128 pixels resulting of the preprocessing downsampling and the usage of 1 channel for the gray scale data.

Figure \ref{fig:cnndecoderoutput} shows the decoder model summary output after compiling the model.

\begin{figure}[ht]
    \centering
    \includegraphics[width = 14cm, height = 8cm]{images/cnn-decoder-output.png}
    \caption[CNN Decoder model summary output]{CNN Decoder model summary output}
    \label{fig:cnndecoderoutput}
\end{figure}

Both of the above-described VAE and VQ-VAE models use this network architecture to encode and decode images where different steps/layers are put in between them to implement the differences of the two models.

\newpage

\begin{figure}[ht]
    \centering
    \includegraphics[width = 14cm, height = 12cm]{images/cnn-vae-encoder-output.png}
    \caption[CNN VAE encoder model summary output]{CNN VAE encoder model summary output}
    \label{fig:cnnvaeencoderoutput}
\end{figure}

\newpage

\begin{figure}[ht]
    \centering
    \includegraphics[width = 14cm, height = 12cm]{images/cnn-vqvae-encoder-output.png}
    \caption[CNN VQ-VAE encoder model summary output]{CNN VQ-VAE encoder model summary output}
    \label{fig:cnnvqvaeencoderoutput}
\end{figure}

% Make sure floats (figures) are placed before new section
\FloatBarrier

\subsubsection{Hidden Layers}

As described, the implemented \acrshort{cnn} is based on Context-encoding Variational Autoencoder for Unsupervised Anomaly Detection paper \cite{cevaemodel} in where Zimmerer et al. describe it as: 

\begin{quote}
    For the encoder and decoder networks, we chose fully convolutional networks with five 2D-Conv-Layers and 2D-Transposed-Conv-Layers respectively with CoordConv, kernel size 4 and stride 2, each layer followed by a Leaky-ReLU non-linearity. The encoder and decoder are symmetric with 16, 64, 256, 1024 feature maps and a latent variable size of 1024
\end{quote}

In project's \acrfull{cnn}, a Normalization layer is added between them resulting in the following composite:

\begin{itemize}
    \item Convolutional neuron (Conv2D or Conv2DTranspose) 
    \item Normalization neuron (BatchNormalization)
    \item ReLu neuron (LeakyReLu)
\end{itemize}

The convolutional layer is paremetrized with 4 kernels, stride 2 and padding 0. The kernel size aims to avoid the network learning too fast, while using stride 2 to lower the training resources needed and train faster.

According to Martin Riva \cite{batchnorm} "Batch Norm is a normalization technique done between the layers of a Neural Network instead of in the raw data. It is done along mini-batches instead of the full data set. It serves to speed up training and use higher learning rates, making learning easier. The standard deviation of the neurons' output.". The project is loading raw data in batches which are normalized as a preparation step, however, normalizing data between each layer might help.

ReLu activation is usually applied in Deep Learning (and so it is in the project) right after every convolutional layer to remove linearity since, basically, all the previous operations are linear. 

\subsection{Flow from directory}

When working with large image datasets data cannot be loaded in memory and then passed to the model for training. Instead, data is required to be processed in batches to alleviate the need of compute resources.

To do so, Keras provides the \href{https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_directory}{Flow from directory} method which allows you to configure data loading parameters and several pre-processing actions.

Once configured, the flow from directory method retrieves an iterator object, which is passed to the model which will invoke it (the iterator) to get the next batch of images when needed during the training steps in each epoch.

Aligning the model batch size with the iterator batch size avoids unnecessary calls and more importantly tunes memory usage while training the models.

The method was paremetrized to rescale the pixels from original 256 to final 128. Training and testing images were extracted from NIFTI files and stored in \acrshort{png} format with original size so that they could be resized as desired at this stage. This configuration allowed the project to experiment with both sizes much easier.

\subsection{Creating artificial images from latent space}
\label{sec:generate-images}
To create artificial images from the latent space the decoder input must be generated. As explained, the decoder expects an object from the latent space, which means that 1024 values are expected in the normal Gaussian distribution. This is randomly generated with NumPy library and its \href{https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html}{numpy.random.normal} function.

In figure \ref{fig:vaes}, samples are created from 6 variables, each of them corresponding to a learnt feature. Two different images are shown created from 2 different samples, yellow and red ones. Yellow and red points shown in the image (inside "Latent distributions") could have perfectly been created as described in the previous paragraph.

In this project, the 1024 generated values are equivalent to those yellow and red points and they define the small variations on brain features that will be used when creating the new image; refer to diagram \ref{fig:image-from-latent-space-diagram} for a clear representation of the pipeline.

\begin{figure}[ht]
    \centering
    \includegraphics[width = 14cm]{images/tfm-vae-latent-space.png}
    \caption[Image created from latent space]{Image created from latent space}
    \label{fig:image-from-latent-space-diagram}
\end{figure}

\subsection{Image reconstruction}

Once that VAE and \acrshort{vqvae} models are created, test images are used for testing purposes.

Testing an image process requires to encode and decode it. When encoding an image, the encoder will create a vector of values (z) which will be passed to the decoder to be reconstructed with the formula:

\begin{equation}
    z = \mu  + e^{(0,5 * \sigma)} * random(vector)
\end{equation}

The random vector is generated with the same method used for creating images from latent space (see section \ref{sec:generate-images}). Hence, image reconstruction is very similar to images created from latent space with the difference that the random values are influenced by the means and variance of the model.